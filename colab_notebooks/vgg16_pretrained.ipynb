{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1621614682898,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "DUpN56XCZL5D",
    "outputId": "6e004702-83e1-447a-d5c8-cdd902e63360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126993,
     "status": "ok",
     "timestamp": 1621614810249,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "nNMG-cNaZ0z6",
    "outputId": "9be9f293-9a23-4322-e1d3-f7113f0cfe68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/images’: File exists\n",
      "mkdir: cannot create directory ‘/content/images/train’: File exists\n",
      "mkdir: cannot create directory ‘/content/images/test’: File exists\n",
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from /content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/train/train.rar\n",
      "\n",
      "\n",
      "Would you like to replace the existing file /content/images/train/images/006316.jpg\n",
      " 30041 bytes, modified on 2020-09-01 11:46\n",
      "with a new one\n",
      " 30041 bytes, modified on 2020-09-01 11:46\n",
      "\n",
      "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
      "\n",
      "\n",
      "Would you like to replace the existing file /content/images/train/images/006331.jpg\n",
      " 24689 bytes, modified on 2020-09-01 11:46\n",
      "with a new one\n",
      " 24689 bytes, modified on 2020-09-01 11:46\n",
      "\n",
      "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
      "\n",
      "\n",
      "Would you like to replace the existing file /content/images/train/images/006335.jpg\n",
      " 57040 bytes, modified on 2020-09-01 11:46\n",
      "with a new one\n",
      " 57040 bytes, modified on 2020-09-01 11:46\n",
      "\n",
      "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
      "\n",
      "\n",
      "Would you like to replace the existing file /content/images/train/images/006346.jpg\n",
      " 24320 bytes, modified on 2020-09-01 11:46\n",
      "with a new one\n",
      " 24320 bytes, modified on 2020-09-01 11:46\n",
      "\n",
      "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit e\n",
      "\n",
      "All OK\n",
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from /content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/test/test.rar\n",
      "\n",
      "\n",
      "Would you like to replace the existing file /content/images/test/images/006452.jpg\n",
      " 33085 bytes, modified on 2020-09-01 09:47\n",
      "with a new one\n",
      " 33085 bytes, modified on 2020-09-01 09:47\n",
      "\n",
      "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit e\n",
      "\n",
      "All OK\n"
     ]
    }
   ],
   "source": [
    "!mkdir \"/content/images\"\n",
    "!mkdir \"/content/images/train\"\n",
    "!mkdir \"/content/images/test\"\n",
    "!unrar x \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/train/train.rar\" \"/content/images/train\"\n",
    "!unrar x \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/test/test.rar\" \"/content/images/test\"\n",
    "!cp \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/train/annotations.json\" \"/content/images/train\"\n",
    "!cp \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/images/test/annotations.json\" \"/content/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1621614810250,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "aIL2RVeNfis1"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from google.colab.patches import cv2_imshow\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "sys.path.append(\"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/scripts/\")\n",
    "from data_generation import DataGeneration\n",
    "from filter_cats import filtered_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2582,
     "status": "ok",
     "timestamp": 1621614812823,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "9nm4h82NhiWe",
    "outputId": "e8b04e56-fcdb-41b3-d7d4-56a538b4f50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n",
      "Requirement already satisfied: segmentation_models in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
      "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation_models) (1.0.8)\n",
      "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation_models) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.1.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "!pip install segmentation_models\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1621614812824,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "FFWtiOW-rsd7"
   },
   "outputs": [],
   "source": [
    "SIZE_X = 128\n",
    "SIZE_Y = 128\n",
    "N_CATS = 16\n",
    "ADD_BACKGROUND = False\n",
    "N_CHANNELS = N_CATS + int(ADD_BACKGROUND)   # One more channel for background\n",
    "\n",
    "INPUT_SHAPE = (SIZE_X, SIZE_Y, 3)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAIN_IMAGES_PATH = \"/content/images/train/images\"\n",
    "TRAIN_ANNOTATIONS_PATH = \"/content/images/train/annotations.json\"\n",
    "TEST_IMAGES_PATH = \"/content/images/test/images\"\n",
    "TEST_ANNOTATIONS_PATH = \"/content/images/test/annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1621614812824,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "szzjlpIkxvgv"
   },
   "outputs": [],
   "source": [
    "def batch_generator(batchsize, images_path, annotation_path):\n",
    "  print(\"Generator created for \" + annotation_path)\n",
    "  i_img = 0\n",
    "  coco = COCO(annotation_path)\n",
    "\n",
    "  categories_ids, categories_names, img_ids = filtered_cats(coco, n=N_CATS)\n",
    "  np.random.shuffle(img_ids)\n",
    "\n",
    "  images = coco.loadImgs(img_ids)\n",
    "  img_paths = [img[\"file_name\"] for img in images]\n",
    "\n",
    "  data_gen = DataGeneration(coco, SIZE_X, SIZE_Y, categories_ids, add_background=ADD_BACKGROUND)\n",
    "\n",
    "  while True:\n",
    "    inputs = np.zeros((batchsize, SIZE_X, SIZE_Y, 3))\n",
    "    outputs = np.zeros((batchsize, SIZE_X, SIZE_Y, N_CHANNELS))\n",
    "\n",
    "    for i in range(batchsize):\n",
    "      try:\n",
    "        inputs[i] = data_gen.x_sample(join(images_path, img_paths[i_img]))\n",
    "        outputs[i] = data_gen.y_sample(img_ids[i_img])\n",
    "      except:\n",
    "        i_img = (i_img + 1) % len(img_paths)\n",
    "        inputs[i] = data_gen.x_sample(join(images_path, img_paths[i_img]))\n",
    "        outputs[i] = data_gen.y_sample(img_ids[i_img])\n",
    "\n",
    "      i_img = (i_img + 1) % len(img_paths)\n",
    "\n",
    "    yield (inputs/255), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1621614812825,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "T8uDnHMchtvI"
   },
   "outputs": [],
   "source": [
    "def get_steps_per_epoch(batchsize, annotation_path):\n",
    "  coco = COCO(annotation_path)\n",
    "  categories_ids, categories_names, img_ids = filtered_cats(coco, n=N_CATS)\n",
    "\n",
    "  return int(math.floor(len(img_ids) / batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1621614812826,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "PVUO7y5vGuJy"
   },
   "outputs": [],
   "source": [
    "def get_class_weights(annotation_path):\n",
    "  class_weights = np.zeros(N_CHANNELS)\n",
    "  coco = COCO(annotation_path)\n",
    "  categories_ids, categories_names, img_ids = filtered_cats(coco, n=N_CATS)\n",
    "\n",
    "  data_gen = DataGeneration(coco, SIZE_X, SIZE_Y, categories_ids, add_background=ADD_BACKGROUND)\n",
    "\n",
    "  for id in img_ids:\n",
    "    weights = data_gen.get_class_weights(id)\n",
    "    if (~np.isnan(weights)).all():\n",
    "      class_weights = np.add(class_weights, weights)\n",
    " \n",
    "  class_weights = np.divide(class_weights, len(img_ids))\n",
    "  return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1621614812827,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "RJzNfs5jtoXi"
   },
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_vgg16_unet(input_shape, n_channels=1):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    \"\"\" Pre-trained VGG16 Model \"\"\"\n",
    "    vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "    vgg16.trainable = False\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = vgg16.get_layer(\"block1_conv2\").output         ## (512 x 512)\n",
    "    s2 = vgg16.get_layer(\"block2_conv2\").output         ## (256 x 256)\n",
    "    s3 = vgg16.get_layer(\"block3_conv3\").output         ## (128 x 128)\n",
    "    s4 = vgg16.get_layer(\"block4_conv3\").output         ## (64 x 64)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = vgg16.get_layer(\"block5_conv3\").output         ## (32 x 32)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
    "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
    "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
    "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
    "\n",
    "    \"\"\" Output \"\"\"\n",
    "    outputs = Conv2D(n_channels, (1,1), padding=\"same\", activation=\"softmax\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"VGG16_U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6546,
     "status": "ok",
     "timestamp": 1621614819366,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "2CATs0MCdM9c",
    "outputId": "6255f370-611c-4afe-c27f-5838a2444719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.80s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/scripts/data_generation.py:95: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ret / total_pixels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "{'water': 0.02348432080781901, 'bread-white': 0.029602669256155838, 'salad-leaf-salad-green': 0.029401972874972496, 'tomato': 0.04941681957890603, 'butter': 0.05976591557209492, 'bread-wholemeal': 0.041174053960029273, 'coffee-with-caffeine': 0.053523662677300354, 'carrot': 0.05028303986196188, 'apple': 0.060862382723465676, 'mixed-vegetables': 0.07445970457757911, 'egg': 0.06761805003964108, 'tea': 0.079701658481245, 'rice': 0.08627239592429911, 'banana': 0.09280022511924023, 'mixed-salad-chopped-without-sauce': 0.09713706136094344, 'cucumber': 0.10449606718434651}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = np.ones(N_CHANNELS)\n",
    "class_weights = np.divide(class_weights, get_class_weights(TRAIN_ANNOTATIONS_PATH))\n",
    "sum = np.sum(class_weights)\n",
    "class_weights = np.divide(class_weights, sum)\n",
    "\n",
    "# class_weights = []\n",
    "\n",
    "coco = COCO(TEST_ANNOTATIONS_PATH)\n",
    "categories_ids, categories_names, img_ids = filtered_cats(coco, n=N_CATS)\n",
    "\n",
    "if ADD_BACKGROUND:\n",
    "  categories_names += [\"background\"]\n",
    "\n",
    "print(dict(zip(categories_names, class_weights)))\n",
    "np.sum(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1621614819366,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "-syBGFvTrbvL",
    "outputId": "caacc8b0-3128-4806-873d-b0be6b0ef570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG16_U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 128, 128, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 128, 128, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 64, 64, 128)  73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 64, 64, 128)  147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 32, 32, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 32, 32, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 32, 32, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 32, 32, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 16, 16, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 16, 16, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 16, 16, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 16, 16, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 8, 8, 512)    0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 8, 8, 512)    2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 8, 8, 512)    2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 8, 8, 512)    2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 16, 16, 512)  1049088     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  4719104     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  2359808     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 256)  524544      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 512)  0           conv2d_transpose_5[0][0]         \n",
      "                                                                 block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 64, 64, 128)  131200      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_6[0][0]         \n",
      "                                                                 block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 128, 128, 64) 32832       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_7[0][0]         \n",
      "                                                                 block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128, 128, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 128, 128, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 64) 256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128, 128, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 1040        activation_15[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 25,863,312\n",
      "Trainable params: 11,144,784\n",
      "Non-trainable params: 14,718,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_vgg16_unet(INPUT_SHAPE, N_CHANNELS)\n",
    "\n",
    "total_loss = sm.losses.DiceLoss(per_image=True, class_weights=class_weights) + sm.losses.BinaryFocalLoss() + sm.losses.JaccardLoss(per_image=True, class_weights=class_weights)\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5, per_image=True), sm.metrics.FScore(threshold=0.5, per_image=True)]\n",
    "\n",
    "model.compile(optimizer='Adam', loss=total_loss, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1621614819367,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "GsleFG9f02re"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "             ModelCheckpoint(\"/content/logs/model_checkpoints/food_segmentation.h5\", verbose=1, save_best_only=True),\n",
    "             EarlyStopping(patience=5, monitor=\"val_loss\"),\n",
    "             TensorBoard(log_dir=\"/content/logs/tensorboard_logs\")\n",
    "]\n",
    "\n",
    "gen_train = batch_generator(BATCH_SIZE, TRAIN_IMAGES_PATH, TRAIN_ANNOTATIONS_PATH)\n",
    "gen_val = batch_generator(BATCH_SIZE, TEST_IMAGES_PATH, TEST_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1678740,
     "status": "ok",
     "timestamp": 1621616498098,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "V24-UbVFt2Ob",
    "outputId": "20e123f8-9a4e-4264-8ab1-fc5332e5f93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Generator created for /content/images/train/annotations.json\n",
      "loading annotations into memory...\n",
      "Done (t=1.59s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch 1/20\n",
      "417/417 [==============================] - ETA: 0s - loss: 2.0058 - iou_score: 0.8609 - f1-score: 0.8628Generator created for /content/images/test/annotations.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "417/417 [==============================] - 215s 510ms/step - loss: 2.0058 - iou_score: 0.8609 - f1-score: 0.8628 - val_loss: 2.0141 - val_iou_score: 0.8488 - val_f1-score: 0.8508\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.01412, saving model to /content/logs/model_checkpoints/food_segmentation.h5\n",
      "Epoch 2/20\n",
      "417/417 [==============================] - 210s 503ms/step - loss: 2.0026 - iou_score: 0.8937 - f1-score: 0.8979 - val_loss: 2.0164 - val_iou_score: 0.8516 - val_f1-score: 0.8555\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.01412\n",
      "Epoch 3/20\n",
      "417/417 [==============================] - 208s 500ms/step - loss: 2.0018 - iou_score: 0.8933 - f1-score: 0.8986 - val_loss: 2.0133 - val_iou_score: 0.8630 - val_f1-score: 0.8666\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.01412 to 2.01334, saving model to /content/logs/model_checkpoints/food_segmentation.h5\n",
      "Epoch 4/20\n",
      "417/417 [==============================] - 208s 499ms/step - loss: 2.0012 - iou_score: 0.8950 - f1-score: 0.9010 - val_loss: 2.0177 - val_iou_score: 0.8540 - val_f1-score: 0.8579\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.01334\n",
      "Epoch 5/20\n",
      "417/417 [==============================] - 208s 499ms/step - loss: 2.0007 - iou_score: 0.8976 - f1-score: 0.9041 - val_loss: 2.0167 - val_iou_score: 0.8528 - val_f1-score: 0.8569\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.01334\n",
      "Epoch 6/20\n",
      "417/417 [==============================] - 207s 497ms/step - loss: 2.0002 - iou_score: 0.8991 - f1-score: 0.9061 - val_loss: 2.0197 - val_iou_score: 0.8342 - val_f1-score: 0.8384\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.01334\n",
      "Epoch 7/20\n",
      "417/417 [==============================] - 209s 501ms/step - loss: 1.9999 - iou_score: 0.8995 - f1-score: 0.9069 - val_loss: 2.0193 - val_iou_score: 0.8315 - val_f1-score: 0.8361\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.01334\n",
      "Epoch 8/20\n",
      "417/417 [==============================] - 208s 500ms/step - loss: 1.9995 - iou_score: 0.9024 - f1-score: 0.9101 - val_loss: 2.0203 - val_iou_score: 0.8333 - val_f1-score: 0.8381\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.01334\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(gen_train,\n",
    "                    verbose=1,\n",
    "                    steps_per_epoch=get_steps_per_epoch(BATCH_SIZE, TRAIN_ANNOTATIONS_PATH),\n",
    "                    epochs=20,\n",
    "                    validation_data=gen_val,\n",
    "                    validation_steps=get_steps_per_epoch(BATCH_SIZE, TEST_ANNOTATIONS_PATH),\n",
    "                    #class_weight=class_weights,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1621616498965,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "8aB9IbG3nU8C"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/logs/model_checkpoints/food_segmentation.h5\" \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/weights/vgg16_pretrained_facu.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1621616498966,
     "user": {
      "displayName": "GONZALO JOAQUIN DAVIDOV",
      "photoUrl": "",
      "userId": "00291866231039682739"
     },
     "user_tz": -120
    },
    "id": "7QoQ7K1Krv-I"
   },
   "outputs": [],
   "source": [
    "!cp -r \"/content/logs/tensorboard_logs\" \"/content/drive/Shareddrives/Intercambio/MATERIAS/Deep Learning/Food Recognition/tensorboard_logs/VGG16\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vgg16_pretrained_gonza.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
